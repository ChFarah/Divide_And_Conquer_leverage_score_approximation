{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled34.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-StrBH59pxM"
      },
      "source": [
        "# Leverage score approximation using Divide And Conquer (DAC) algorithm.\n",
        "\n",
        "\n",
        "We present here an example of Nyström approximation for a random data matrix, using an RBF kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeGRqrLokOdc"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import *\n",
        "import sys\n",
        "from scipy.linalg import svd\n",
        "from math import *\n",
        "import os\n",
        "import scipy.linalg as spl\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyNOOf3DEWtx"
      },
      "source": [
        "def DAC(X, lambda_, sample_size, kernel_function, kernel_param):\n",
        "  \"\"\"\n",
        "  This function computes an approximation of the ridge leverage score, using a divide and conquer strategy.\n",
        "  X: numpy array of size (n, d) where n is the number of data and d number of features.\n",
        "  lambda_: regularisation term.\n",
        "  sample_size: size of sub-matrix.\n",
        "  kernel_function: a function that compute the kernel matrix, in the same form as the functions of the sklearn.metrics.pairwise library.\n",
        "  kernel_param: the parameter of the kernel function (the degree if polynomial kernel for example).\n",
        "  \"\"\"\n",
        "\n",
        "  if (lambda_ < 0 or kernel_param < 0):\n",
        "    print(\"lambda_ and kernel_param have to be positive\")\n",
        "    exit(1)\n",
        "\n",
        "  n = X.shape[0]\n",
        "  ind = np.arange(n)\n",
        "  approximated_ls = np.zeros((n))\n",
        "  np.random.shuffle(ind)\n",
        "\n",
        "\n",
        "  for l in range(0, ceil(n/sample_size)):\n",
        "    true_sample_size = min(sample_size, n - l*sample_size)\n",
        "    temp_ind = ind[l*sample_size: l*sample_size + true_sample_size]\n",
        "    K_S = kernel_function(X[temp_ind], X[temp_ind], kernel_param)\n",
        "    a = np.sum(K_S * np.linalg.inv(K_S + lambda_ * np.eye(true_sample_size)) , axis = 1)\n",
        "    approximated_ls[temp_ind] = a\n",
        "\n",
        "  return approximated_ls\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZhO2NnoRlg"
      },
      "source": [
        "\n",
        "def kernel_approximation_fit(X, S, kernel_function, kernel_param):\n",
        "  \"\"\"\n",
        "  This function approximates the kernel matrix K_{i, j} = k(x_i, x_j), using only the data indexed by S\n",
        "\n",
        "  X: numpy array of size (n, d) where n is the number of data and d number of features.\n",
        "  S: list of selected data indices for the Nystrom approximation.\n",
        "  kernel_function: a function that compute the kernel matrix, in the same form as the functions of the library sklearn.metrics.pairwise.\n",
        "  kernel_param: the parameter of the kernel function (the degree if polynomial kernel for example).\n",
        "\n",
        "\n",
        "  Sortie:\n",
        "  mapped_X: numpy array of size (n, |S|)\n",
        "  inv_K_s: numpy array of size (|S|, |S|)\n",
        "  \"\"\"\n",
        "\n",
        "  K_1_1 = kernel_function(X[S], X[S], kernel_param)\n",
        "\n",
        "  # sqrt of kernel matrix on basis vectors\n",
        "  U, sigma, V = svd(K_1_1)\n",
        "  sigma = np.maximum(sigma, 1e-12)\n",
        "\n",
        "  # compute K_{1, 1}^{-1/2}\n",
        "  normalization_ = np.dot(U / np.sqrt(sigma), V) \n",
        "\n",
        "  embedded = kernel_function(X, X[S], kernel_param)\n",
        "\n",
        "  X_mapped = np.dot(embedded, normalization_.T)\n",
        "  return X_mapped, normalization_\n",
        "\t\n",
        "\t\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFhLtJWWGdfc"
      },
      "source": [
        "#generate random matrix and compute the kernel matrix\n",
        "n, d = 20, 2\n",
        "X = np.random.normal(size=(n, d))\n",
        "\n",
        "#compute approximated leverage scores using DAC algorithm\n",
        "kernel_parameter = 1\n",
        "lambda_ = 1\n",
        "\n",
        "#the size of each sub-matrix is equal to \\sqrt{n}. The size of the last one is n - int(\\fract{n}{\\sqrt{n}})*int(\\sqrt{n})\n",
        "sample_size = 5\n",
        "approximated_leverage_scores = DAC(X, lambda_, sample_size, rbf_kernel, kernel_parameter)\n",
        "\n",
        "#sample 10% of data proportionally to these scores\n",
        "selected = np.random.choice(n, size=int(0.1*n), replace=False, p=approximated_leverage_scores/np.sum(approximated_leverage_scores))\n",
        "\n",
        "#compute Nyström approximation suing the sampled data\n",
        "X_mapped, inv_K_s = kernel_approximation_fit(X, selected, rbf_kernel, kernel_parameter)\n",
        "approximated_K = np.dot(X_mapped, X_mapped.T)\n"
      ],
      "execution_count": 58,
      "outputs": []
    }
  ]
}